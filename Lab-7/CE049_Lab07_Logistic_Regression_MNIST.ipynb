{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d9c70dbb",
      "metadata": {
        "id": "d9c70dbb"
      },
      "source": [
        "## Name: Gopani Kenil G. \n",
        "## Roll No: CE049 \n",
        "## ID: 19CEUOS080\n",
        "##  Lab-7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "512af11d",
      "metadata": {
        "id": "512af11d"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e4c23b79",
      "metadata": {
        "id": "e4c23b79"
      },
      "outputs": [],
      "source": [
        "# Setting Up Hyperparameters and Data Set Parameters\n",
        "\n",
        "num_classes = 10 # 0 to 9 digits\n",
        "num_features = 784 # 28*28\n",
        "learning_rate = 0.01\n",
        "training_steps = 1000\n",
        "batch_size = 256\n",
        "display_step = 51"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "bce7641a",
      "metadata": {
        "id": "bce7641a"
      },
      "outputs": [],
      "source": [
        "# Loading and Preparing the MNIST Data Set\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
        "\n",
        "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
        "x_train, x_test = x_train / 255., x_test / 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d9e0a724",
      "metadata": {
        "id": "d9e0a724"
      },
      "outputs": [],
      "source": [
        "# Shuffling and Batching the Data\n",
        "\n",
        "train_data=tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
        "\n",
        "train_data=train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "6e420093",
      "metadata": {
        "id": "6e420093"
      },
      "outputs": [],
      "source": [
        "# Initializing Weights and Biases\n",
        "\n",
        "W = tf.Variable(tf.ones([num_features, num_classes]), name=\"weight\")\n",
        "b = tf.Variable(tf.zeros([num_classes]), name=\"bias\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "093659fd",
      "metadata": {
        "id": "093659fd"
      },
      "outputs": [],
      "source": [
        "# Defining Logistic Regression and Cost Function\n",
        "\n",
        "def logistic_regression(x):\n",
        "    return tf.nn.softmax(tf.matmul(x, W) + b)\n",
        "\n",
        "def cross_entropy(y_pred, y_true):\n",
        "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
        "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "f7e43dd5",
      "metadata": {
        "id": "f7e43dd5"
      },
      "outputs": [],
      "source": [
        "# Defining Optimizers and Accuracy Metrics\n",
        "\n",
        "def accuracy(y_pred, y_true):\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
        "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "optimizer = tf.optimizers.SGD(learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "73a99b7b",
      "metadata": {
        "id": "73a99b7b"
      },
      "outputs": [],
      "source": [
        "# Optimization Process and Updating Weights and Biases\n",
        "\n",
        "def run_optimization(x, y):\n",
        "    with tf.GradientTape() as g:\n",
        "        pred = logistic_regression(x)\n",
        "        loss = cross_entropy(pred, y)\n",
        "    gradients = g.gradient(loss, [W, b])\n",
        "    optimizer.apply_gradients(zip(gradients, [W, b]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "7bfbeca5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bfbeca5",
        "outputId": "471d4f7e-d69e-408b-8c10-aba09f7725fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 51, loss: 538.663635, accuracy: 0.769531\n",
            "step: 102, loss: 102.817978, accuracy: 0.890625\n",
            "step: 153, loss: 224.749939, accuracy: 0.828125\n",
            "step: 204, loss: 74.882690, accuracy: 0.921875\n",
            "step: 255, loss: 91.350616, accuracy: 0.910156\n",
            "step: 306, loss: 138.076248, accuracy: 0.859375\n",
            "step: 357, loss: 66.994141, accuracy: 0.933594\n",
            "step: 408, loss: 614.191833, accuracy: 0.851562\n",
            "step: 459, loss: 56.102993, accuracy: 0.933594\n",
            "step: 510, loss: 90.010178, accuracy: 0.902344\n",
            "step: 561, loss: 88.799721, accuracy: 0.917969\n",
            "step: 612, loss: 110.771179, accuracy: 0.910156\n",
            "step: 663, loss: 70.352547, accuracy: 0.929688\n",
            "step: 714, loss: 81.328079, accuracy: 0.910156\n",
            "step: 765, loss: 90.933945, accuracy: 0.925781\n",
            "step: 816, loss: 71.745239, accuracy: 0.921875\n",
            "step: 867, loss: 104.933945, accuracy: 0.917969\n",
            "step: 918, loss: 44.586540, accuracy: 0.964844\n",
            "step: 969, loss: 50.028866, accuracy: 0.937500\n"
          ]
        }
      ],
      "source": [
        "# The Training Loop\n",
        "\n",
        "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
        "    run_optimization(batch_x, batch_y)\n",
        "    if step % display_step == 0:\n",
        "        pred = logistic_regression(batch_x)\n",
        "        loss = cross_entropy(pred, batch_y)\n",
        "        acc = accuracy(pred, batch_y)\n",
        "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "271fd473",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "271fd473",
        "outputId": "d9f1a999-900c-494c-8eab-ad30d5ecd50e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.912400\n"
          ]
        }
      ],
      "source": [
        "# Testing Model Accuracy Using the Test Data\n",
        "\n",
        "pred = logistic_regression(x_test)\n",
        "print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "5c750bcb",
      "metadata": {
        "id": "5c750bcb"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "CE049_Lab07_Logistic_Regression_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}